import os
import json
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import shutil
from pathlib import Path
from tqdm import tqdm
from datetime import datetime

# ===========================
# ğŸ›ï¸ ì‚¬ìš©ì íŒŒë¼ë¯¸í„° ì¡°ì ˆ ì˜ì—­
# ===========================
SIMILARITY_THRESHOLD = 0.82  # ì •ë°€ë„ê°€ ë†’ì•„ì¡Œìœ¼ë¯€ë¡œ 0.82~0.85 ì¶”ì²œ
TARGET_DATE_FOLDER = datetime.now().strftime("%Y-%m-%d") 
BATCH_SIZE = 256 # 3ì¤‘ ì—°ì‚°ì´ë¯€ë¡œ 512ë³´ë‹¤ ë‚®ì¶°ì„œ ì•ˆì •ì„± í™•ë³´
QUARANTINE_FOLDER = "duplicates_storage" 
QUARANTINE_JSON_LOG = os.path.join(QUARANTINE_FOLDER, "quarantined_json_data.json")
# ===========================

# ëª¨ë¸ ì„¤ì • (í”„ë¡œì íŠ¸ í´ë” ë‚´ ì €ì¥)
project_root = Path.cwd()
model_dir = project_root / "models"
model_dir.mkdir(parents=True, exist_ok=True)
torch.hub.set_dir(str(model_dir))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
model = nn.Sequential(*list(model.children())[:-1]) 
model.to(device).eval()

# â­ [ì •ë°€ ë¶„ì„] 3ê°€ì§€ ë‹¤ë¥¸ ì‹œì ì˜ ì „ì²˜ë¦¬ ì •ì˜
# 1. ì¤‘ì•™ ì§‘ì¤‘ (Center Crop)
transform_center = transforms.Compose([
    transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
# 2. ì „ì²´ êµ¬ë„ (Full View)
transform_full = transforms.Compose([
    transforms.Resize((224, 224)), transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
# 3. í™•ëŒ€ ë¶„ì„ (Detailed Zoom)
transform_zoom = transforms.Compose([
    transforms.Resize(400), transforms.CenterCrop(224), transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

def extract_features_multi_pass(image_paths, model, device, batch_size):
    """í•œ ì´ë¯¸ì§€ë‹¹ 3ë²ˆì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ í‰ê· ì„ ëƒ…ë‹ˆë‹¤."""
    final_features = []
    valid_paths = []
    
    for i in tqdm(range(0, len(image_paths), batch_size), desc="ğŸ” ì´ë¯¸ì§€ 3ì¤‘ ì •ë°€ ë¶„ì„ ì¤‘"):
        batch_files = image_paths[i:i+batch_size]
        
        # ê° ì „ì²˜ë¦¬ ë°©ì‹ë³„ íŠ¹ì§• ë³´ê´€ìš©
        batch_f1, batch_f2, batch_f3 = [], [], []
        
        for p in batch_files:
            try:
                img = Image.open(p).convert('RGB')
                # 3ê°€ì§€ ì‹œì ìœ¼ë¡œ ì´ë¯¸ì§€ ë³€í™˜
                batch_f1.append(transform_center(img))
                batch_f2.append(transform_full(img))
                batch_f3.append(transform_zoom(img))
                valid_paths.append(p)
            except: continue
            
        if not batch_f1: continue
        
        with torch.no_grad():
            # 3ë²ˆì˜ ë¶„ì„ ì‹¤í–‰
            t1 = torch.stack(batch_f1).to(device)
            t2 = torch.stack(batch_f2).to(device)
            t3 = torch.stack(batch_f3).to(device)
            
            out1 = torch.flatten(model(t1), 1).cpu().numpy()
            out2 = torch.flatten(model(t2), 1).cpu().numpy()
            out3 = torch.flatten(model(t3), 1).cpu().numpy()
            
            # â­ [ì¤‘ìš”] 3ê°œ ë²¡í„°ì˜ í‰ê· ì„ ë‚´ì–´ 'ê°•ë ¥í•œ íŠ¹ì§•' ìƒì„±
            # $$ \mathbf{f}_{final} = \frac{\mathbf{f}_{center} + \mathbf{f}_{full} + \mathbf{f}_{zoom}}{3} $$
            avg_features = (out1 + out2 + out3) / 3.0
            final_features.append(avg_features)
            
    return valid_paths, np.vstack(final_features) if final_features else ([], np.array([]))

def move_to_quarantine(image_path_str):
    """ì´ë¯¸ì§€ ê²©ë¦¬ ë° JSON ë°±ì—… ë¡œì§"""
    try:
        os.makedirs(QUARANTINE_FOLDER, exist_ok=True)
        img_p = Path(image_path_str)
        filename = img_p.name
        category_dir = img_p.parent.parent 
        category_name = category_dir.name
        json_file_path = category_dir / f"{category_name}_data.json"
        
        # íŒŒì¼ ì´ë™ (ì¹´í…Œê³ ë¦¬ëª…_ID.jpg)
        dest_filename = f"{category_name}_{filename}"
        dest_path = os.path.join(QUARANTINE_FOLDER, dest_filename)
        shutil.move(image_path_str, dest_path)

        # JSON ì •ì œ
        target_id = img_p.stem 
        if json_file_path.exists():
            temp_lines = []
            quarantined_data = None
            with open(json_file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        if data.get("id") == target_id:
                            quarantined_data = data
                            continue
                        temp_lines.append(line)
                    except: temp_lines.append(line)
            
            with open(json_file_path, "w", encoding="utf-8") as f:
                f.writelines(temp_lines)
            
            if quarantined_data:
                quarantined_data["quarantined_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                with open(QUARANTINE_JSON_LOG, "a", encoding="utf-8") as f:
                    f.write(json.dumps(quarantined_data, ensure_ascii=False) + "\n")
    except Exception as e:
        print(f"âš ï¸ ê²©ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print(f"ğŸš€ [3ì¤‘ ë¶„ì„] AI ì¤‘ë³µ ì œê±°ê¸° ê°€ë™ | ëŒ€ìƒ: {TARGET_DATE_FOLDER}")
    
    # 1. ì´ë¯¸ì§€ ìˆ˜ì§‘ (ë‚ ì§œ/ì¹´í…Œê³ ë¦¬/thumbnails/*.jpg)
    all_image_paths = [str(p) for p in Path(TARGET_DATE_FOLDER).glob("**/thumbnails/*.jpg")]
    print(f"ğŸ“¸ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€: {len(all_image_paths)}ê°œ")
    
    if len(all_image_paths) < 2:
        print("âœ… ë¶„ì„í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤."); exit()

    # 2. 3PASS íŠ¹ì§• ì¶”ì¶œ
    valid_paths, features = extract_features_multi_pass(all_image_paths, model, device, BATCH_SIZE)

    # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    from sklearn.preprocessing import normalize
    features = normalize(features, axis=1) 
    sim_matrix = np.matmul(features, features.T) 

    # 4. ì¤‘ë³µ íŒë³„
    deleted_indices = set()
    for i in range(len(valid_paths)):
        if i in deleted_indices: continue
        for j in range(i + 1, len(valid_paths)):
            if j not in deleted_indices and sim_matrix[i][j] >= SIMILARITY_THRESHOLD:
                deleted_indices.add(j)

    # 5. ê²©ë¦¬ ì‹¤í–‰
    print(f"\nğŸ“¦ {len(deleted_indices)}ê°œì˜ ì¤‘ë³µ ì˜ì‹¬ ë°ì´í„°ë¥¼ ê²©ë¦¬í•©ë‹ˆë‹¤.")
    for idx in sorted(list(deleted_indices), reverse=True):
        move_to_quarantine(valid_paths[idx])

    print(f"ğŸ‰ 3ì¤‘ ì •ë°€ ë¶„ì„ ë° ì •ì œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")