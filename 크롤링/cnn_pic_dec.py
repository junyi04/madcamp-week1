import os
import json
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import cv2
from pathlib import Path
from tqdm import tqdm
from datetime import datetime
from facenet_pytorch import MTCNN, InceptionResnetV1
from sklearn.metrics.pairwise import cosine_similarity
import mysql.connector
from mysql.connector import Error

MYSQL_CONFIG = {
    'host': 'localhost',
    'database': 'madcamp1_db',
    'user': 'root',
    'password': '4038'
}

SERVER_DOMAIN = "young-forty.ngrok.app"

def save_duplicates_to_mysql(quarantine_tasks, filtered_date):
    """ì¤‘ë³µ ì´ë¯¸ì§€ë¥¼ MySQLì— ì €ì¥"""
    try:
        connection = mysql.connector.connect(**MYSQL_CONFIG)
        cursor = connection.cursor()
        
        sql = """INSERT INTO filtered_duplicates 
                 (id, duplicate_id, original_id, original_path, title, author, 
                  views, likes, category, url, image_url, similarity_score, filtered_date) 
                 VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                 ON DUPLICATE KEY UPDATE 
                 similarity_score=VALUES(similarity_score)"""
        
        for task in quarantine_tasks:
            log = task['log_item']
            dup_path = Path(log['duplicate_path'])
            
            # ì´ë¯¸ì§€ URL (ê²©ë¦¬ì†Œ)
            image_url = f"https://{SERVER_DOMAIN}/duplicates_storage/DUP_{dup_path.name}"
            
            cursor.execute(sql, (
                f"dup_{log['duplicate_id']}",
                log['duplicate_id'],
                log['original_id'],
                log['original_path'],
                "ì¤‘ë³µ ì´ë¯¸ì§€",  # ì œëª©ì€ ì›ë³¸ JSONì—ì„œ ê°€ì ¸ì˜¤ë©´ ë” ì¢‹ìŒ
                "",
                0,
                0,
                log['category'],
                "",
                image_url,
                log['score'],
                filtered_date
            ))
        
        connection.commit()
        print(f"ğŸ’¾ MySQL ì €ì¥: ì¤‘ë³µ ì´ë¯¸ì§€ {len(quarantine_tasks)}ê±´")
        
    except Error as e:
        print(f"âŒ MySQL ì—ëŸ¬: {e}")
    finally:
        if connection and connection.is_connected():
            cursor.close()
            connection.close()
# ===========================
# ğŸ›ï¸ RTX 4060 í™˜ê²½ ë° ì •ë°€ë„ ì„¤ì •
# ===========================
SIMILARITY_THRESHOLD = 0.92 
W_COSINE = 0.3 
W_FACE = 0.7 
TARGET_DATE_FOLDER = datetime.now().strftime("%Y-%m-%d") 
QUARANTINE_FOLDER = "duplicates_storage" 
QUARANTINE_JSON_LOG = os.path.join(QUARANTINE_FOLDER, "quarantined_json_data.json")

project_root = Path.cwd()
model_cache_dir = project_root / "models_cache"
model_cache_dir.mkdir(parents=True, exist_ok=True)
os.environ['TORCH_HOME'] = str(model_cache_dir)
torch.hub.set_dir(str(model_cache_dir))
# ===========================

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ëª¨ë¸ ë¡œë“œ (FaceNet/MTCNN ê¸°ë°˜ ì‹œê°í™” ì¤€ë¹„)
resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
resnet50 = nn.Sequential(*list(resnet50.children())[:-1]).to(device).eval()
mtcnn = MTCNN(keep_all=False, device=device, post_process=False) 
facenet = InceptionResnetV1(pretrained='vggface2').to(device).eval()

# ğŸ¨ 3-Pass ì „ì²˜ë¦¬ (ê¸°ëŠ¥ ìœ ì§€ 100%)
preprocess = {
    "center": transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])]),
    "full": transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])]),
    "zoom": transforms.Compose([transforms.Resize(400), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])])
}

def get_face_info(img_p):
    """ì–¼êµ´ ì„ë² ë”©ê³¼ ì‹œê°í™”ìš© ë°•ìŠ¤ ì¢Œí‘œ ì¶”ì¶œ"""
    try:
        img = Image.open(img_p).convert('RGB')
        boxes, _ = mtcnn.detect(img)
        if boxes is not None:
            face = mtcnn(img)
            if face is not None:
                emb = facenet(face.unsqueeze(0).to(device)).detach().cpu().numpy()
                return emb, boxes[0], True
    except: pass
    return None, None, False

def move_and_visualize_with_log(src_path, dest_path, box, log_item):
    """[ë³µêµ¬] í•œê¸€ ê²½ë¡œ ëŒ€ì‘ ë° ì–¼êµ´ ë°•ìŠ¤ ì‹œê°í™” ë¡œì§"""
    os.makedirs(QUARANTINE_FOLDER, exist_ok=True)
    
    # ğŸ“ í•œê¸€ ê²½ë¡œ ëŒ€ì‘ ë¡œë“œ
    try:
        img_array = np.fromfile(str(src_path), np.uint8)
        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
    except: img = None
    
    if img is not None:
        if box is not None:
            # ì–¼êµ´ì— ì´ˆë¡ìƒ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)
            cv2.putText(img, "AI DUPLICATE", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # í•œê¸€ ê²½ë¡œ ëŒ€ì‘ ì €ì¥
        _, img_encoded = cv2.imencode('.jpg', img)
        img_encoded.tofile(str(dest_path))
        if os.path.exists(src_path): os.remove(src_path)
    
    # ìƒì„¸ ë¹„êµ ë¡œê·¸ ê¸°ë¡
    with open(QUARANTINE_JSON_LOG, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_item, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    print(f"ğŸš€ [RTX 4060] ë¹„êµ ë°ì´í„° ê¸°ë¡ ëª¨ë“œ ì •ì œ ì‹œì‘")
    all_paths = [str(p) for p in Path(TARGET_DATE_FOLDER).glob("**/thumbnails/*.jpg")]
    if len(all_paths) < 2: exit()

    valid_paths, res_feats, face_data = [], [], {}
    for p in tqdm(all_paths, desc="ğŸ§  íŠ¹ì§• ì¶”ì¶œ ë° ë¶„ì„"):
        try:
            img = Image.open(p).convert('RGB')
            # 1. ResNet 3-Pass ì¶”ì¶œ [ë³µêµ¬]
            with torch.no_grad():
                f_avg = (resnet50(preprocess["center"](img).unsqueeze(0).to(device)) +
                         resnet50(preprocess["full"](img).unsqueeze(0).to(device)) +
                         resnet50(preprocess["zoom"](img).unsqueeze(0).to(device))) / 3.0
                res_feats.append(torch.flatten(f_avg, 1).cpu().numpy())
            
            # 2. ì–¼êµ´ ì •ë³´(ì„ë² ë”© + ì¢Œí‘œ)
            emb, box, ok = get_face_info(p)
            if ok: face_data[p] = {"emb": emb, "box": box}
            valid_paths.append(p)
        except: continue

    res_feats = np.vstack(res_feats)
    res_sim_matrix = cosine_similarity(res_feats)

    deleted_indices = set()
    quarantine_tasks = []
    deleted_info_by_cat = {}

    for i in range(len(valid_paths)):
        if i in deleted_indices: continue
        for j in range(i + 1, len(valid_paths)):
            if j in deleted_indices: continue
            
            sim_res = res_sim_matrix[i][j]
            pi, pj = valid_paths[i], valid_paths[j]
            
            # ê°€ì¤‘ì¹˜ íŒë³„ (FaceNet ë°˜ì˜)
            if pi in face_data and pj in face_data:
                sim_face = cosine_similarity(face_data[pi]["emb"], face_data[pj]["emb"])[0][0]
                final_score = (sim_res * W_COSINE) + (sim_face * W_FACE)
            else:
                final_score = sim_res

            if final_score >= SIMILARITY_THRESHOLD:
                deleted_indices.add(j)
                p_pi, p_pj = Path(pi), Path(pj)
                cat = p_pj.parent.parent.name
                
                log_item = {
                    "duplicate_id": p_pj.stem,
                    "duplicate_path": pj,
                    "original_id": p_pi.stem,
                    "original_path": pi,
                    "score": round(float(final_score), 4),
                    "category": cat,
                    "processed_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                
                quarantine_tasks.append({
                    "src": pj, "box": face_data.get(pj, {}).get("box"), "log_item": log_item
                })

                if cat not in deleted_info_by_cat: deleted_info_by_cat[cat] = []
                deleted_info_by_cat[cat].append(p_pj.stem)

    # ğŸ“¦ íŒŒì¼ ì´ë™ ë° ì‹œê°í™” ìˆ˜í–‰
    print(f"ğŸ“¦ {len(deleted_indices)}ê°œ ì¤‘ë³µ ê²©ë¦¬ ì¤‘ (ë¹„êµ ë¡œê·¸ ìƒì„±)")
    for task in quarantine_tasks:
        dest = Path(QUARANTINE_FOLDER) / f"DUP_{Path(task['src']).name}"
        move_and_visualize_with_log(task['src'], dest, task['box'], task['log_item'])
    
    if quarantine_tasks:
        save_duplicates_to_mysql(quarantine_tasks, TARGET_DATE_FOLDER)

    # ğŸ’¾ [ë³µêµ¬] ì¹´í…Œê³ ë¦¬ë³„ JSON ê°±ì‹  ë° ì œëª© ì—†ìŒ ë³´ì •
    for cat, d_ids in deleted_info_by_cat.items():
        j_path = Path(TARGET_DATE_FOLDER) / cat / f"{cat}_data.json"
        if j_path.exists():
            with open(j_path, "r", encoding="utf-8") as f:
                data_list = json.load(f)
            # ì¤‘ë³µ ID ì œê±°
            new_list = [item for item in data_list if item.get("id") not in d_ids]
            # ì œëª© ë³´ì •
            for item in new_list:
                if not item.get('title') or item['title'].strip() == "":
                    item['title'] = "ì œëª© ì—†ìŒ"
            with open(j_path, "w", encoding="utf-8") as f:
                json.dump(new_list, f, ensure_ascii=False, indent=2)

    print("ğŸ‰ ëª¨ë“  ì •ë°€ ì •ì œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")